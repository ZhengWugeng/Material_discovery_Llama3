{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token xxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bed7b0f21e241e6979ba7c5412070b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa8ccf4fe2944bcfaac9ae3f603a4ca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af94abc1cba141c2a3b117dfef9495b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "342c73d65f4b458481b5d6b81e39b29e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ed4f02a245145c19796c02dcf07be4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "336b9482c2a74b5694813b28373122cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6413c0645ea457884c9db520f4a1ff7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd11ff34a876442cb7d5d76a9396cc35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52195c6670db41d4b47f44d7e74df6c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/177 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "867fec9eabf74d08a6b1120c89d5d43b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97a246c8d55d47a3ba4b54e852e119cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed958e7c34b8440eadd9505f61c684b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'meta-llama/Meta-Llama-3-8B'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             #load_in_8bit=True,\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             device_map=\"auto\"\n",
    "                                            )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8eff95060dd43928eaae3aadc69250a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73f16125ff06411bbb94538f0a197645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.25M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f99b395bf2434180b936c65145805e53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/10.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a6944042fab45009fe8473e6d14d44c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/149k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "400a83fe989741e5ba9000555348d176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/439 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f07ad0e5b96b4d98aad6735ddbc028fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/682 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "648462aa75914ec695421fe1f0dfb7d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/2.43G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             device_map=\"auto\"\n",
    "                                            )\n",
    "tokenizer = AutoTokenizer.from_pretrained('ZWG817/Llama3_Chat_Materials')\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model.load_adapter('ZWG817/Llama3_Chat_Materials')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"ZWG817/Abstract\")\n",
    "data_train = data[\"train\"]\n",
    "print(data_train)\n",
    "\n",
    "#custom_data = load_dataset('json', data_files='data_eval.json')\n",
    "#data_val = custom_data['train']\n",
    "\n",
    "with open('materials.txt', 'r') as file:\n",
    "    word_list = file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = load_dataset('csv', data_files=\"gdc.csv\")\n",
    "# data_train = concatenate_datasets([data_train, data[\"train\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = {\n",
    "    'title': word_list,\n",
    "    'abstract': [s.replace('_', '') for s in word_list],\n",
    "    'id': ['material'] * len(word_list),  # 假设新数据集中没有id信息\n",
    "    'publicationDate': [None] * len(word_list)  # 假设新数据集中没有publicationDate信息\n",
    "}\n",
    "new_dataset = Dataset.from_dict(new_data)\n",
    "\n",
    "data_train = concatenate_datasets([data_train, new_dataset])\n",
    "\n",
    "new_data = {\n",
    "    'title': [s.replace('_', '') for s in word_list],\n",
    "    'abstract': word_list,\n",
    "    'id': ['material'] * len(word_list),  # 假设新数据集中没有id信息\n",
    "    'publicationDate': [None] * len(word_list)  # 假设新数据集中没有publicationDate信息\n",
    "}\n",
    "new_dataset = Dataset.from_dict(new_data)\n",
    "\n",
    "data_train = concatenate_datasets([data_train, new_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val = data_train.select(range(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(type_, prompt, output=None, eos_token=\"</s>\"):\n",
    "    begin = \"<s>[INST]\"\n",
    "    if type_ == 'material':\n",
    "        instruction = \"<<SYS>> As a helpful scientific assistant versed in the composition of various materials, identify and elaborate on the components that constitute the following material.<</SYS>>\\n\"\n",
    "        prompt = f\"{prompt} is [/INST]\\n\"\n",
    "        output = f\"{output + ' ' + eos_token if output else ''} \"\n",
    "    elif type_ == 'gilbert':\n",
    "        instruction = \"<<SYS>> Tasked as a helpful scientific assistant, provide a concise numerical value in response to the subsequent inquiry. Refrain from including any supplementary information or context.<</SYS>>\\n\"\n",
    "        prompt = f\"The Gilbert damping constant of {prompt}[/INST]\\n\"\n",
    "        output = f\"{str(output) + ' ' + eos_token if output else ''} \"\n",
    "    elif type_ == 'summary':\n",
    "        instruction = \"<<SYS>> Functioning as a helpful scientific assistant, distill the content of the ensuing paper into a succinct summary that captures the essential findings and conclusions.<</SYS>>\\n\"\n",
    "        prompt = f\"The Gilbert damping constant of {prompt}[/INST]\\n\"\n",
    "        output = f\"{str(output) + ' ' + eos_token if output else ''} \"\n",
    "    else:\n",
    "        instruction = \"<<SYS>> In your role as a helpful scientific assistant, convey the abstract of the forthcoming paper, presenting the key objectives, methodology, results, and implications in a clear and concise manner.<</SYS>>\\n\"\n",
    "        prompt = f\"{prompt} [/INST]\\n\"\n",
    "        output = f\"Abstract: {output + ' ' + eos_token if output else ''} \"\n",
    "    #end = \"[/INST]\\n\"\n",
    "    prompt = (\" \").join([str(begin), str(instruction), str(input), str(output)])\n",
    "    return prompt\n",
    "\n",
    "print(generate_prompt(data_train[1][\"id\"], data_train[1][\"title\"], data_train[1][\"abstract\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompt = generate_prompt(data_train[-1][\"id\"], data_train[-1][\"title\"])\n",
    "input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "with torch.cuda.amp.autocast():\n",
    "  generation_output = model.generate(\n",
    "      input_ids=input_tokens,\n",
    "      max_new_tokens=128,\n",
    "      do_sample=True,\n",
    "      top_k=10,\n",
    "      top_p=0.9,\n",
    "      temperature=0.3,\n",
    "      repetition_penalty=1.15,\n",
    "      num_return_sequences=1,\n",
    "      eos_token_id=tokenizer.eos_token_id,\n",
    "      pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "op = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
    "print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "        r=128,\n",
    "        lora_alpha=256,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Add new tokens to the tokenizer\n",
    "num_added_toks = tokenizer.add_tokens(word_list)\n",
    "print(f\"Added {num_added_toks} tokens\")\n",
    "\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<PAD>\"})\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "#model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Freeze all parameters in the model\n",
    "#for param in model.parameters():\n",
    "#    param.requires_grad = False\n",
    "\n",
    "embeddings = model.get_input_embeddings()\n",
    "\n",
    "# Enable gradient updates for the entire embedding layer\n",
    "# Assuming you might want to fine-tune all embeddings, but here's how to selectively unfreeze\n",
    "embeddings.weight.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='results',            # 输出目录\n",
    "    num_train_epochs=2,              # 总训练轮数\n",
    "    per_device_train_batch_size=4,   # 训练的batch size\n",
    "    per_device_eval_batch_size=4,    # 验证的batch size\n",
    "    gradient_accumulation_steps=4, \n",
    "    #gradient_checkpointing=True,\n",
    "    #optim = \"paged_adamw_32bit\",\n",
    "    optim = \"adamw_torch\",\n",
    "    bf16=True,\n",
    "    #fp16=True,\n",
    "    warmup_steps=300,                # 预热步数\n",
    "    learning_rate = 1e-4,\n",
    "    max_grad_norm = 0.2,\n",
    "    #max_steps = 50,\n",
    "    #warmup_ratio = 0.03,\n",
    "    #weight_decay=0.01,               # 权重衰减\n",
    "    save_strategy=\"steps\",           # 设置保存策略为\"steps\"\n",
    "    save_steps=300,                  # 每500步保存一次模型\n",
    "    save_total_limit=3,              # 最多保存3个检查点\n",
    "    evaluation_strategy=\"epoch\",     # 设置评估策略为\"steps\"\n",
    "    group_by_length=True,\n",
    "    #eval_steps=10000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient checkpointing enabling\n",
    "model.enable_input_require_grads()\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(prompt):\n",
    "  output = []\n",
    "\n",
    "  for a, d, s in zip(prompt[\"id\"], prompt[\"title\"], prompt[\"abstract\"]):\n",
    "    op = generate_prompt(a, d, s)\n",
    "    output.append(op)\n",
    "\n",
    "  return output\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=data_train,\n",
    "    eval_dataset=data_val,\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=formatting_func,\n",
    "    max_seq_length=512,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args\n",
    ")\n",
    "\n",
    "# We will also pre-process the model by upcasting the layer norms in float 32 for more stable training\n",
    "#for name, module in trainer.model.named_modules():\n",
    "#    if \"norm\" in name:\n",
    "#        module = module.to(torch.float32)\n",
    "\n",
    "trainer.train()\n",
    "# trainer.save_model(f\"{output_dir}/final\")\n",
    "\n",
    "# Step Training Loss Validation Loss\n",
    "# 10 1.848200 1.746341\n",
    "# 20 1.688300 1.696681\n",
    "# 30 1.654500 1.698127\n",
    "# 40 1.579400 1.652010\n",
    "# 50 1.492600 1.701877"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(f\"/home/final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f65a6605fbaf447c8556676efd8ff7dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4480c4baf7d54a2e9aa52516f845c52f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/2.43G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/ZWG817/Llama3_Chat_Materials/commit/ab65b3842f5caff5643d730eaef65f9011e0f3bb', commit_message='Upload tokenizer', commit_description='', oid='ab65b3842f5caff5643d730eaef65f9011e0f3bb', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"ZWG817/Llama3_Chat_Materials\")\n",
    "tokenizer.push_to_hub(\"ZWG817/Llama3_Chat_Materials\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = load_dataset('json', data_files='selected_paragraphs.json')\n",
    "# data = data['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(content):\n",
    "    begin = \"<s>[INST]\"\n",
    "    #syst = \"<<SYS>> You are a helpful assistant, always answer as helpfully as possible.\\n If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.<</SYS>>\\n\"\n",
    "    #inst = \"Read the following text. Does it mention the Gilbert damping constant of a certain material? If so, list the corresponding material and its Gilbert damping canstant.\\n\" + content\n",
    "    syst = \"<<SYS>> Acting as a helpful assistant with a focus on efficiency, review the ensuing text to determine if it references the Gilbert damping constant for a specific material. Should such information be present, list only the material involved alongside its respective Gilbert damping constant.<</SYS>>\\n\"\n",
    "    inst = content\n",
    "    end = \"[/INST]\\n\"\n",
    "    prompt = (\" \").join([str(begin), str(syst), str(inst), str(end)])\n",
    "    return prompt\n",
    "\n",
    "print(generate_prompt(str(data['train'][0]['abstract'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data['train']:\n",
    "    try:\n",
    "        input_prompt = generate_prompt(str(i['abstract']))\n",
    "        input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "        with torch.cuda.amp.autocast():\n",
    "            generation_output = model.generate(\n",
    "                input_ids=input_tokens,\n",
    "                max_new_tokens=128,\n",
    "                do_sample=True,\n",
    "                top_k=5,\n",
    "                top_p=0.9,\n",
    "                temperature=0.2,\n",
    "                repetition_penalty=1.1,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        op = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
    "        #print(op)\n",
    "        \n",
    "        inst_index = op.find('[/INST]')\n",
    "        \n",
    "        if inst_index != -1:\n",
    "            print(op[inst_index + len('[/INST]'):])\n",
    "        else:\n",
    "            print(\"未找到'[/INST]'标记\")\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000,  45147,  31868,  65562,     60,   1134,  39031,   2511,  73789,\n",
       "            439,    264,  11190,  18328,    449,    264,   5357,    389,  15374,\n",
       "             11,   3477,    279,  72758,   1495,    311,   8417,    422,    433,\n",
       "          15407,    279,  46092,  84614,   6926,    369,    264,   3230,   3769,\n",
       "             13,  12540,   1778,   2038,    387,   3118,     11,   1160,   1193,\n",
       "            279,   3769,   6532,  16662,   1202,  20081,  46092,  84614,   6926,\n",
       "          16134,    524,  39031,  40171,   5783,  22974,    532,    423,   4341,\n",
       "            278,   9451,  89492,   3893,   5364,  13915,   7911,  16628,    320,\n",
       "             72,  32989,      8,    304,    958,  10546,   9473,    198,  23045,\n",
       "          50185,    320,   5484,     34,      8,  52170,     14,   7489,   6535,\n",
       "             90,    508,  32816,   6251,   6535,     90,   1399,  32816,     33,\n",
       "           6535,     90,    508,     92,   8693,     16,     13,    717,    198,\n",
       "          20211,   5738,  99725,     14,   7489,   6535,     90,    508,  32816,\n",
       "           6251,   6535,     90,   1399,  32816,     33,   6535,     90,    508,\n",
       "             92,   8693,     16,     13,    717,  26807,      8,   6067,    617,\n",
       "           1027,  20041,    198,   1820,  18673,   2740,    323,   9526,    750,\n",
       "             13,  60288,   1113,   6205,  33297,  21037,    706,   1027,   1511,\n",
       "            198,    998,   6767,    872,  33297,   2065,    520,  50843,    323,\n",
       "            872,    958,  10546,   9473,    198,  23045,  12127,  18508,     13,\n",
       "           4314,  15629,    527,   1766,    311,    387,    315,    459,   3276,\n",
       "          11691,    442,  39100,   7138,    198,   2000,    279,  27313,  29014,\n",
       "           2134,  26839,    320,     15,     13,     20,     12,     16,  26807,\n",
       "            570,  11205,   8915,  24924,    198,  13495,   1051,  20041,   1701,\n",
       "          67744,    283,    258,   3177,  72916,    320,     33,   7416,      8,\n",
       "          15105,     13,    578,    198,     33,   7416,  22323,  16805,  38617,\n",
       "           2536,  60272,  49889,   5531,  12903,  17301,  54743,     13,    763,\n",
       "            198,  85324,    311,    279,  29217,    369,   8045,  59402,    358,\n",
       "           7650,   3623,   6251,     33,  13931,     11,    420,    198,  87803,\n",
       "           2536,   2827,  49889,   9103,    374,  29014,  26839,    323,   8617,\n",
       "          59086,   8333,    198,  38655,     13,  15636,     11,    311,  10552,\n",
       "            279,  22772,  17432,     11,    264,  32887,    198,   2590,   3196,\n",
       "            389,    279,  77933,   3834,    459,    285,    354,  18237,   6811,\n",
       "           1990,    279,    198,  15205,    323,   1948,   3623,   6251,     33,\n",
       "          13931,    706,   1027,   8040,     13,   1226,   1501,    430,    279,\n",
       "          29014,  26839,    198,  91746,    315,    279,  12903,  12330,   2536,\n",
       "          60272,  49889,   9103,    374,   1664,  55717,    555,  13126,    264,\n",
       "            198,  18568,    602,  32989,    323,   2204,  77933,    958,  22974,\n",
       "            532,    459,    285,    354,  18237,   5151,   1990,    198,   1820,\n",
       "           1948,    323,   5740,   3623,   6251,     33,  13931,     13,   1115,\n",
       "            459,    285,    354,  18237,   6811,    706,   1027,  11007,    198,\n",
       "           1729,    279,   8990,    315,    279,   3623,   6251,     33,  26839,\n",
       "          44393,    315,   7524,    198,     76,  64333,   2065,    315,  52170,\n",
       "             14,   7489,   6251,     33,  19945,     84,    323,  29014,     14,\n",
       "           7489,   6251,     33,  10482,     70,     46,   3927,  13931,     11,\n",
       "           1405,    264,  13790,    198,  30998,   9073,    706,   1027,  13468,\n",
       "             13,  66028,  65562,    933]], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('result', save_embedding_layers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "# for param in model.parameters():\n",
    "#     print(param.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(134933, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=128, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=128, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=128, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=128, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=134933, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     print(param.requires_grad, name, param.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify which parameters are trainable\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(f\"Trainable: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"Trainable: {name}\", param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.add_special_tokens({\"pad_token\": \"<PAD>\"})\n",
    "# model.resize_token_embeddings(len(tokenizer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
