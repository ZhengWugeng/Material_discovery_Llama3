{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d46b4ec90fb64c2e89af03132847079b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dbce1164d9844be80011d5d213b031b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ca2b8f94f1240c1aab2b9bd0187bc60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "623cb2f4615c45c0adcb95e3b848588a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a69a8980632b4505855459d98259ffbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6125ef91efd84cd98cc92d653f65c88c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c524beea1b249cbbb5610daf3e34a27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bc20b3e81dd4462b96e249287963048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf50ce2d4f0548c3ba53315cc97b69ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/177 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40379516f5814e45ad6ee2c092cdd655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1538c29c38ed4223924a4ff4edb0bc8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6b2f8d81d2548029288e7355c2a8b2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'meta-llama/Meta-Llama-3-8B'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             #load_in_8bit=True,\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             device_map=\"auto\"\n",
    "                                            )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8beda9a9877b48be9071f82e6d0d3974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c10ea943a74b4a658174ea569eb8986b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.25M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c7f070075854088a473b71168a608fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/10.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9c64d3c599a4bb6bc3555eef2fc5d0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/149k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "409cc1998f13468f9ddd320584221e0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/439 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f48f2a6f849841b9b03c1f6adc374385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/682 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5beb5d3205974af8b2b6c3443d79fefd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/2.43G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "#                                              torch_dtype=torch.bfloat16,\n",
    "#                                              device_map=\"auto\"\n",
    "#                                             )\n",
    "# tokenizer = AutoTokenizer.from_pretrained('ZWG817/Llama3_Chat_Materials')\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# model.load_adapter('ZWG817/Llama3_Chat_Materials')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dd134deb50d433eabc187637b5f2617",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['publicationDate', 'title', 'abstract', 'id'],\n",
      "    num_rows: 387655\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset(\"ZWG817/Abstract\")\n",
    "data_train = data[\"train\"]\n",
    "print(data_train)\n",
    "\n",
    "#custom_data = load_dataset('json', data_files='data_eval.json')\n",
    "#data_val = custom_data['train']\n",
    "\n",
    "with open('materials.txt', 'r') as file:\n",
    "    word_list = file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = load_dataset('csv', data_files=\"gdc.csv\")\n",
    "# data_train = concatenate_datasets([data_train, data[\"train\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = {\n",
    "    'title': word_list,\n",
    "    'abstract': [s.replace('_', '') for s in word_list],\n",
    "    'id': ['material'] * len(word_list),  # 假设新数据集中没有id信息\n",
    "    'publicationDate': [None] * len(word_list)  # 假设新数据集中没有publicationDate信息\n",
    "}\n",
    "new_dataset = Dataset.from_dict(new_data)\n",
    "\n",
    "data_train = concatenate_datasets([data_train, new_dataset])\n",
    "\n",
    "new_data = {\n",
    "    'title': [s.replace('_', '') for s in word_list],\n",
    "    'abstract': word_list,\n",
    "    'id': ['material'] * len(word_list),  # 假设新数据集中没有id信息\n",
    "    'publicationDate': [None] * len(word_list)  # 假设新数据集中没有publicationDate信息\n",
    "}\n",
    "new_dataset = Dataset.from_dict(new_data)\n",
    "\n",
    "data_train = concatenate_datasets([data_train, new_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val = data_train.select(range(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>> In your role as a helpful scientific assistant, convey the abstract of the forthcoming paper, presenting the key objectives, methodology, results, and implications in a clear and concise manner.<</SYS>>\n",
      " <bound method Kernel.raw_input of <ipykernel.ipkernel.IPythonKernel object at 0x7fb74ea161d0>> Abstract: Magnetization dynamics in W/CoFeB, CoFeB/Pt and W/CoFeB/Pt multilayers was\n",
      "investigated using spin-orbit-torque ferromagnetic resonance (SOT-FMR)\n",
      "technique. An analytical model based on magnetization dynamics due to SOT was\n",
      "used to fit heavy metal (HM) thickness dependence of symmetric and\n",
      "antisymmetric components of the SOT-FMR signal. The analysis resulted in a\n",
      "determination of the properties of HM layers, such as spin Hall angle and spin\n",
      "diffusion length. The spin Hall angle of -0.36 and 0.09 has been found in the\n",
      "W/CoFeB and CoFeB/Pt bilayers, respectively, which add up in the case of\n",
      "W/CoFeB/Pt trilayer. More importantly, we have determined effective interfacial\n",
      "spin-orbit fields at both W/CoFeB and CoFeB/Pt interfaces, which are shown to\n",
      "cancel Oersted field for particular thicknesses of the heavy metal layers,\n",
      "leading to pure spin-current-induced dynamics and indicating the possibility\n",
      "for a more efficient magnetization switching. </s> \n"
     ]
    }
   ],
   "source": [
    "def generate_prompt(type_, prompt, output=None, eos_token=\"</s>\"):\n",
    "    begin = \"<s>[INST]\"\n",
    "    if type_ == 'material':\n",
    "        instruction = \"<<SYS>> As a helpful scientific assistant versed in the composition of various materials, identify and elaborate on the components that constitute the following material.<</SYS>>\\n\"\n",
    "        prompt = f\"{prompt} is [/INST]\\n\"\n",
    "        output = f\"{output + ' ' + eos_token if output else ''} \"\n",
    "    elif type_ == 'gilbert':\n",
    "        instruction = \"<<SYS>> Tasked as a helpful scientific assistant, provide a concise numerical value in response to the subsequent inquiry. Refrain from including any supplementary information or context.<</SYS>>\\n\"\n",
    "        prompt = f\"The Gilbert damping constant of {prompt}[/INST]\\n\"\n",
    "        output = f\"{str(output) + ' ' + eos_token if output else ''} \"\n",
    "    elif type_ == 'summary':\n",
    "        instruction = \"<<SYS>> Functioning as a helpful scientific assistant, distill the content of the ensuing paper into a succinct summary that captures the essential findings and conclusions.<</SYS>>\\n\"\n",
    "        prompt = f\"The Gilbert damping constant of {prompt}[/INST]\\n\"\n",
    "        output = f\"{str(output) + ' ' + eos_token if output else ''} \"\n",
    "    else:\n",
    "        instruction = \"<<SYS>> In your role as a helpful scientific assistant, convey the abstract of the forthcoming paper, presenting the key objectives, methodology, results, and implications in a clear and concise manner.<</SYS>>\\n\"\n",
    "        prompt = f\"{prompt} [/INST]\\n\"\n",
    "        output = f\"Abstract: {output + ' ' + eos_token if output else ''} \"\n",
    "    #end = \"[/INST]\\n\"\n",
    "    prompt = (\" \").join([str(begin), str(instruction), str(input), str(output)])\n",
    "    return prompt\n",
    "\n",
    "print(generate_prompt(data_train[1][\"id\"], data_train[1][\"title\"], data_train[1][\"abstract\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>> As a helpful scientific assistant versed in the composition of various materials, identify and elaborate on the components that constitute the following material.<</SYS>>\n",
      " <bound method Kernel.raw_input of <ipykernel.ipkernel.IPythonKernel object at 0x7fb74ea161d0>>  Ba_2Zn_2Fe_12-xCr_xO_22 </s>  <a href=\"/wiki/FeCo_2BO_5 \" title = Ba_2Co_xZn_yFe_1-x-yFe_12O_22 </s> Co2FeAl </td>  Mn3O4 </td>  Fe2As2 </td>  Ni1-xCoxCr2S4 </td>  CuMnGe3O8 </td>  LaNi5 </td>  BaCo2V2O9 </s>  Rb_2FeF_4 </td> Ba_2Zn_2Fe_12-xCr_xO_22 </td>  Ba_2Zn_2Fe_12-xCr_xO_22 </td>  MnNb_3S_6 </td> Co_60Fe_20Dy_20 </td>  CaCu3Ti2O12 </s>  V_1-xCr_xO_2 </\n"
     ]
    }
   ],
   "source": [
    "input_prompt = generate_prompt(data_train[-1][\"id\"], data_train[-1][\"title\"])\n",
    "input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "with torch.cuda.amp.autocast():\n",
    "  generation_output = model.generate(\n",
    "      input_ids=input_tokens,\n",
    "      max_new_tokens=128,\n",
    "      do_sample=True,\n",
    "      top_k=10,\n",
    "      top_p=0.9,\n",
    "      temperature=0.3,\n",
    "      repetition_penalty=1.15,\n",
    "      num_return_sequences=1,\n",
    "      eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "op = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
    "print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "        r=128,\n",
    "        lora_alpha=256,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 6676 tokens\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Add new tokens to the tokenizer\n",
    "num_added_toks = tokenizer.add_tokens(word_list)\n",
    "print(f\"Added {num_added_toks} tokens\")\n",
    "\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<PAD>\"})\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "#model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Freeze all parameters in the model\n",
    "#for param in model.parameters():\n",
    "#    param.requires_grad = False\n",
    "\n",
    "embeddings = model.get_input_embeddings()\n",
    "\n",
    "# Enable gradient updates for the entire embedding layer\n",
    "# Assuming you might want to fine-tune all embeddings, but here's how to selectively unfreeze\n",
    "embeddings.weight.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='results',            # 输出目录\n",
    "    num_train_epochs=2,              # 总训练轮数\n",
    "    per_device_train_batch_size=4,   # 训练的batch size\n",
    "    per_device_eval_batch_size=4,    # 验证的batch size\n",
    "    gradient_accumulation_steps=4, \n",
    "    #gradient_checkpointing=True,\n",
    "    #optim = \"paged_adamw_32bit\",\n",
    "    optim = \"adamw_torch\",\n",
    "    bf16=True,\n",
    "    #fp16=True,\n",
    "    warmup_steps=300,                # 预热步数\n",
    "    learning_rate = 1e-4,\n",
    "    max_grad_norm = 0.2,\n",
    "    #max_steps = 50,\n",
    "    #warmup_ratio = 0.03,\n",
    "    #weight_decay=0.01,               # 权重衰减\n",
    "    save_strategy=\"steps\",           # 设置保存策略为\"steps\"\n",
    "    save_steps=300,                  # 每500步保存一次模型\n",
    "    save_total_limit=3,              # 最多保存3个检查点\n",
    "    evaluation_strategy=\"epoch\",     # 设置评估策略为\"steps\"\n",
    "    group_by_length=True,\n",
    "    #eval_steps=10000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient checkpointing enabling\n",
    "model.enable_input_require_grads()\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function SFTTrainer._prepare_non_packed_dataloader.<locals>.tokenize at 0x7f75dc6cfd90> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2f4bb72133c4099bd57e9acc5f8b266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13365 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0ec9492c0d94d618fdaaed00dc769e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1670' max='1670' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1670/1670 35:06, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.505200</td>\n",
       "      <td>0.418196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.184600</td>\n",
       "      <td>0.285515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1670, training_loss=0.2917763738575096, metrics={'train_runtime': 2109.1939, 'train_samples_per_second': 12.673, 'train_steps_per_second': 0.792, 'total_flos': 9.247513863212237e+16, 'train_loss': 0.2917763738575096, 'epoch': 2.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def formatting_func(prompt):\n",
    "  output = []\n",
    "\n",
    "  for a, d, s in zip(prompt[\"id\"], prompt[\"title\"], prompt[\"abstract\"]):\n",
    "    op = generate_prompt(a, d, s)\n",
    "    output.append(op)\n",
    "\n",
    "  return output\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=data_train,\n",
    "    eval_dataset=data_val,\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=formatting_func,\n",
    "    max_seq_length=512,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args\n",
    ")\n",
    "\n",
    "# We will also pre-process the model by upcasting the layer norms in float 32 for more stable training\n",
    "#for name, module in trainer.model.named_modules():\n",
    "#    if \"norm\" in name:\n",
    "#        module = module.to(torch.float32)\n",
    "\n",
    "trainer.train()\n",
    "# trainer.save_model(f\"{output_dir}/final\")\n",
    "\n",
    "# Step Training Loss Validation Loss\n",
    "# 10 1.848200 1.746341\n",
    "# 20 1.688300 1.696681\n",
    "# 30 1.654500 1.698127\n",
    "# 40 1.579400 1.652010\n",
    "# 50 1.492600 1.701877"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(f\"/home/final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f65a6605fbaf447c8556676efd8ff7dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4480c4baf7d54a2e9aa52516f845c52f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/2.43G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/ZWG817/Llama3_Chat_Materials/commit/ab65b3842f5caff5643d730eaef65f9011e0f3bb', commit_message='Upload tokenizer', commit_description='', oid='ab65b3842f5caff5643d730eaef65f9011e0f3bb', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"ZWG817/Llama3_Chat_Materials\")\n",
    "tokenizer.push_to_hub(\"ZWG817/Llama3_Chat_Materials\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = load_dataset('json', data_files='selected_paragraphs.json')\n",
    "# data = data['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(content):\n",
    "    begin = \"<s>[INST]\"\n",
    "    #syst = \"<<SYS>> You are a helpful assistant, always answer as helpfully as possible.\\n If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.<</SYS>>\\n\"\n",
    "    #inst = \"Read the following text. Does it mention the Gilbert damping constant of a certain material? If so, list the corresponding material and its Gilbert damping canstant.\\n\" + content\n",
    "    syst = \"<<SYS>> Acting as a helpful assistant with a focus on efficiency, review the ensuing text to determine if it references the Gilbert damping constant for a specific material. Should such information be present, list only the material involved alongside its respective Gilbert damping constant.<</SYS>>\\n\"\n",
    "    inst = content\n",
    "    end = \"[/INST]\\n\"\n",
    "    prompt = (\" \").join([str(begin), str(syst), str(inst), str(end)])\n",
    "    return prompt\n",
    "\n",
    "print(generate_prompt(str(data['train'][0]['abstract'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data['train']:\n",
    "    try:\n",
    "        input_prompt = generate_prompt(str(i['abstract']))\n",
    "        input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "        with torch.cuda.amp.autocast():\n",
    "            generation_output = model.generate(\n",
    "                input_ids=input_tokens,\n",
    "                max_new_tokens=128,\n",
    "                do_sample=True,\n",
    "                top_k=5,\n",
    "                top_p=0.9,\n",
    "                temperature=0.2,\n",
    "                repetition_penalty=1.1,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        op = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
    "        #print(op)\n",
    "        \n",
    "        inst_index = op.find('[/INST]')\n",
    "        \n",
    "        if inst_index != -1:\n",
    "            print(op[inst_index + len('[/INST]'):])\n",
    "        else:\n",
    "            print(\"未找到'[/INST]'标记\")\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000,  45147,  31868,  65562,     60,   1134,  39031,   2511,  73789,\n",
       "            439,    264,  11190,  18328,    449,    264,   5357,    389,  15374,\n",
       "             11,   3477,    279,  72758,   1495,    311,   8417,    422,    433,\n",
       "          15407,    279,  46092,  84614,   6926,    369,    264,   3230,   3769,\n",
       "             13,  12540,   1778,   2038,    387,   3118,     11,   1160,   1193,\n",
       "            279,   3769,   6532,  16662,   1202,  20081,  46092,  84614,   6926,\n",
       "          16134,    524,  39031,  40171,   5783,  22974,    532,    423,   4341,\n",
       "            278,   9451,  89492,   3893,   5364,  13915,   7911,  16628,    320,\n",
       "             72,  32989,      8,    304,    958,  10546,   9473,    198,  23045,\n",
       "          50185,    320,   5484,     34,      8,  52170,     14,   7489,   6535,\n",
       "             90,    508,  32816,   6251,   6535,     90,   1399,  32816,     33,\n",
       "           6535,     90,    508,     92,   8693,     16,     13,    717,    198,\n",
       "          20211,   5738,  99725,     14,   7489,   6535,     90,    508,  32816,\n",
       "           6251,   6535,     90,   1399,  32816,     33,   6535,     90,    508,\n",
       "             92,   8693,     16,     13,    717,  26807,      8,   6067,    617,\n",
       "           1027,  20041,    198,   1820,  18673,   2740,    323,   9526,    750,\n",
       "             13,  60288,   1113,   6205,  33297,  21037,    706,   1027,   1511,\n",
       "            198,    998,   6767,    872,  33297,   2065,    520,  50843,    323,\n",
       "            872,    958,  10546,   9473,    198,  23045,  12127,  18508,     13,\n",
       "           4314,  15629,    527,   1766,    311,    387,    315,    459,   3276,\n",
       "          11691,    442,  39100,   7138,    198,   2000,    279,  27313,  29014,\n",
       "           2134,  26839,    320,     15,     13,     20,     12,     16,  26807,\n",
       "            570,  11205,   8915,  24924,    198,  13495,   1051,  20041,   1701,\n",
       "          67744,    283,    258,   3177,  72916,    320,     33,   7416,      8,\n",
       "          15105,     13,    578,    198,     33,   7416,  22323,  16805,  38617,\n",
       "           2536,  60272,  49889,   5531,  12903,  17301,  54743,     13,    763,\n",
       "            198,  85324,    311,    279,  29217,    369,   8045,  59402,    358,\n",
       "           7650,   3623,   6251,     33,  13931,     11,    420,    198,  87803,\n",
       "           2536,   2827,  49889,   9103,    374,  29014,  26839,    323,   8617,\n",
       "          59086,   8333,    198,  38655,     13,  15636,     11,    311,  10552,\n",
       "            279,  22772,  17432,     11,    264,  32887,    198,   2590,   3196,\n",
       "            389,    279,  77933,   3834,    459,    285,    354,  18237,   6811,\n",
       "           1990,    279,    198,  15205,    323,   1948,   3623,   6251,     33,\n",
       "          13931,    706,   1027,   8040,     13,   1226,   1501,    430,    279,\n",
       "          29014,  26839,    198,  91746,    315,    279,  12903,  12330,   2536,\n",
       "          60272,  49889,   9103,    374,   1664,  55717,    555,  13126,    264,\n",
       "            198,  18568,    602,  32989,    323,   2204,  77933,    958,  22974,\n",
       "            532,    459,    285,    354,  18237,   5151,   1990,    198,   1820,\n",
       "           1948,    323,   5740,   3623,   6251,     33,  13931,     13,   1115,\n",
       "            459,    285,    354,  18237,   6811,    706,   1027,  11007,    198,\n",
       "           1729,    279,   8990,    315,    279,   3623,   6251,     33,  26839,\n",
       "          44393,    315,   7524,    198,     76,  64333,   2065,    315,  52170,\n",
       "             14,   7489,   6251,     33,  19945,     84,    323,  29014,     14,\n",
       "           7489,   6251,     33,  10482,     70,     46,   3927,  13931,     11,\n",
       "           1405,    264,  13790,    198,  30998,   9073,    706,   1027,  13468,\n",
       "             13,  66028,  65562,    933]], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('result', save_embedding_layers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(134933, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=128, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=128, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=128, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=128, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=134933, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     print(param.requires_grad, name, param.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify which parameters are trainable\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(f\"Trainable: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"Trainable: {name}\", param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.add_special_tokens({\"pad_token\": \"<PAD>\"})\n",
    "# model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LlamaForCausalLM' object has no attribute 'logic'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/peft_model.py:540\u001b[0m, in \u001b[0;36mPeftModel.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# defer to nn.Module's logic\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PeftModelForCausalLM' object has no attribute 'logic'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/tuners/lora/model.py:311\u001b[0m, in \u001b[0;36mLoraModel.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# defer to nn.Module's logic\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LoraModel' object has no attribute 'logic'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogic\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/peft_model.py:542\u001b[0m, in \u001b[0;36mPeftModel.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(name)  \u001b[38;5;66;03m# defer to nn.Module's logic\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m--> 542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/tuners/lora/model.py:313\u001b[0m, in \u001b[0;36mLoraModel.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(name)  \u001b[38;5;66;03m# defer to nn.Module's logic\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m--> 313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LlamaForCausalLM' object has no attribute 'logic'"
     ]
    }
   ],
   "source": [
    "model.logic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
