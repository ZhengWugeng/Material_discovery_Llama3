{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token hf_123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "196385150c7241f2ac909c30b9b8d480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'meta-llama/Meta-Llama-3-8B'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             #load_in_8bit=True,\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             device_map=\"auto\"\n",
    "                                            )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76e3d48cca3045e482b787f2026d13dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             device_map=\"auto\"\n",
    "                                            )\n",
    "tokenizer = AutoTokenizer.from_pretrained('ZWG817/Llama3_Chat_Materials')\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model.load_adapter('ZWG817/Llama3_Chat_Materials')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['publicationDate', 'title', 'abstract', 'id'],\n",
      "    num_rows: 8001\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset(\"ZWG817/Abstract_Template\")\n",
    "data_train = data[\"train\"]\n",
    "print(data_train)\n",
    "\n",
    "#custom_data = load_dataset('json', data_files='data_eval.json')\n",
    "#data_val = custom_data['train']\n",
    "\n",
    "with open('materials.txt', 'r') as file:\n",
    "    word_list = file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In your role as a helpful scientific assistant, Read the following text and summarize the following. Please make sure the material's molecular formulas are involved:\n",
      " Contact resistances between organic semiconductors and metals can dominate\n",
      "the transport properties of electronic devices incorporating such materials. We\n",
      "report measurements of the parasitic contact resistance and the true channel\n",
      "resistance in bottom contact poly(3-hexylthiophene) (P3HT) field-effect\n",
      "transistors with channel lengths from 400 nm up to 40 $\\mu$m, from room\n",
      "temperature down to 77 K. For fixed gate voltage, the ratio of contact to\n",
      "channel resistance decreases with decreasing temperature. We compare this\n",
      "result with a recent model for metal-organic semiconductor contacts. Mobilities\n",
      "corrected for this contact resistance can approach 1 cm$^{2}$/Vs at room\n",
      "temperature and high gate voltages.\n",
      " Summary:\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "def generate_prompt(dialogue, summary=None, eos_token=\"</s>\"):\n",
    "  instruction = \"In your role as a helpful scientific assistant, Read the following text and summarize the following. Please make sure the material's molecular formulas are involved:\\n\"\n",
    "  input = f\"{dialogue}\\n\"\n",
    "  summary = f\"Summary:\\n {summary + ' ' + eos_token if summary else ''} \"\n",
    "  prompt = (\" \").join([instruction, input, summary])\n",
    "  return prompt\n",
    "\n",
    "print(generate_prompt(data_train[4]['abstract']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In your role as a helpful scientific assistant, Read the following text and summarize the following. Please make sure the material's molecular formulas are involved:\n",
      " Phase change superlattice is one of the emerging material technologies for\n",
      "ultralow-power phase change memories. However, the resistance switching\n",
      "mechanism of phase change superlattice is still hotly debated. Early electrical\n",
      "measurements and recent materials characterizations have suggested that the\n",
      "Kooi phase is very likely to be the as-fabricated low-resistance state. Due to\n",
      "the difficulty in in-situ characterization at atomic resolution, the structure\n",
      "of the electrically switched superlattice in its high-resistance state is still\n",
      "unknown and mainly investigated by theoretical modellings. So far, there has\n",
      "been no simple model that can unify experimental results obtained from\n",
      "device-level electrical measurements and atomic-level materials\n",
      "characterizations. In this work, we carry out atomistic transport modellings of\n",
      "the phase change superlattice device and propose a simple mechanism accounting\n",
      "for its high resistance. The modeled high-resistance state is based on the\n",
      "interfacial phase changed superlattice that has previously been mistaken for\n",
      "the low-resistance state. This work advances the understanding of phase change\n",
      "superlattice for emerging memory applications.\n",
      " Summary:\n",
      "     1)   A new resistive switching mechanism is proposed for GeTe-based\n",
      "phase-change superlattices (PCLs).\n",
      "   2)   The interfacial phase transition between Kooi and rhombohedral phases\n",
      "is responsible for the high-resistance state.\n",
      "   3)   The PCL exhibits an intrinsic bistability with two different\n",
      "resistances depending on whether or not the interface is in equilibrium.\n",
      "   4)   The current-induced phase transition occurs via nucleation and growth\n",
      "processes.\n",
      "   5)   The critical size of the nucleus decreases exponentially with increasing\n",
      "temperature.\n",
      "   6)   The temperature-dependent activation energy barrier is consistent with\n",
      "experimental observations. \n"
     ]
    }
   ],
   "source": [
    "input_prompt = generate_prompt(data_train[6]['abstract'])\n",
    "input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "with torch.cuda.amp.autocast():\n",
    "  generation_output = model.generate(\n",
    "      input_ids=input_tokens,\n",
    "      max_new_tokens=1000,\n",
    "      do_sample=True,\n",
    "      top_k=10,\n",
    "      top_p=0.9,\n",
    "      temperature=0.3,\n",
    "      repetition_penalty=1.15,\n",
    "      num_return_sequences=1,\n",
    "      eos_token_id=tokenizer.eos_token_id,\n",
    "      pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "op = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
    "print(op.split(\"</\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1)   A new resistive switching mechanism is proposed for GeTe-based\n",
      "phase-change superlattices (PCLs).\n",
      "   2)   The interfacial phase transition between Kooi and rhombohedral phases\n",
      "is responsible for the high-resistance state.\n",
      "   3)   The PCL exhibits an intrinsic bistability with two different\n",
      "resistances depending on whether or not the interface is in equilibrium.\n",
      "   4)   The current-induced phase transition occurs via nucleation and growth\n",
      "processes.\n",
      "   5)   The critical size of the nucleus decreases exponentially with increasing\n",
      "temperature.\n",
      "   6)   The temperature-dependent activation energy barrier is consistent with\n",
      "experimental observations. \n"
     ]
    }
   ],
   "source": [
    "result = op.split(\"</\")[0]\n",
    "# print(result)\n",
    "content = data_train[6]['abstract']\n",
    "output = result.split(\"Summary:\\n\")[-1]\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "results = []\n",
    "df = pd.DataFrame({\n",
    "    'Number':[],\n",
    "    'Content':[],\n",
    "    'Output':[]\n",
    "})\n",
    "for i,j in enumerate(data_train):\n",
    "    print(i)\n",
    "    input_prompt = generate_prompt(data_train[i]['abstract'])\n",
    "    input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "    with torch.cuda.amp.autocast():\n",
    "      generation_output = model.generate(\n",
    "          input_ids=input_tokens,\n",
    "          max_new_tokens=1000,\n",
    "          do_sample=True,\n",
    "          top_k=10,\n",
    "          top_p=0.9,\n",
    "          temperature=0.3,\n",
    "          repetition_penalty=1.15,\n",
    "          num_return_sequences=1,\n",
    "          eos_token_id=tokenizer.eos_token_id,\n",
    "          pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    op = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
    "    # print(op.split(\"</\")[0])\n",
    "    result = op.split(\"</\")[0]\n",
    "    content = data_train[i]['abstract']\n",
    "    output = result.split(\"Summary:\\n\")[-1]\n",
    "    df.loc[len(df.index)] = [i, content, output]\n",
    "    \n",
    "    if i >= 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number</th>\n",
       "      <th>Content</th>\n",
       "      <th>Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>The resistivity and magnetoresistance measurem...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The influence of electrical and thermal contac...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Controlled bipolar resistive switching (BRS) h...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Grain boundaries (GBs) in metals usually incre...</td>\n",
       "      <td>The resistance of copper grain boundaries d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Contact resistances between organic semiconduc...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>The resistivity of a superconductor in its nor...</td>\n",
       "      <td>1) A novel method is proposed to determine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Phase change superlattice is one of the emergi...</td>\n",
       "      <td>1) A new model is proposed to explain the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>We study magneto-transport properties in singl...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Corrosion has a wide impact on society, causin...</td>\n",
       "      <td>1) High entropy alloys (HEAs) have been wid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>An increase in the quality factor of supercond...</td>\n",
       "      <td>1.   A high-quality-factor (Q-value) SR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>Large area graphene sheets grown by chemical v...</td>\n",
       "      <td>A 2D resistance network model was developed...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Number                                            Content  \\\n",
       "0        0  The resistivity and magnetoresistance measurem...   \n",
       "1        1  The influence of electrical and thermal contac...   \n",
       "2        2  Controlled bipolar resistive switching (BRS) h...   \n",
       "3        3  Grain boundaries (GBs) in metals usually incre...   \n",
       "4        4  Contact resistances between organic semiconduc...   \n",
       "5        5  The resistivity of a superconductor in its nor...   \n",
       "6        6  Phase change superlattice is one of the emergi...   \n",
       "7        7  We study magneto-transport properties in singl...   \n",
       "8        8  Corrosion has a wide impact on society, causin...   \n",
       "9        9  An increase in the quality factor of supercond...   \n",
       "10      10  Large area graphene sheets grown by chemical v...   \n",
       "\n",
       "                                               Output  \n",
       "0                                                 ...  \n",
       "1                                                 ...  \n",
       "2                                                 ...  \n",
       "3      The resistance of copper grain boundaries d...  \n",
       "4                                                 ...  \n",
       "5      1) A novel method is proposed to determine ...  \n",
       "6        1) A new model is proposed to explain the...  \n",
       "7                                                 ...  \n",
       "8      1) High entropy alloys (HEAs) have been wid...  \n",
       "9          1.   A high-quality-factor (Q-value) SR...  \n",
       "10     A 2D resistance network model was developed...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('molecular formula.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000,  45147,  31868,  65562,     60,   1134,  39031,   2511,  73789,\n",
       "            439,    264,  11190,  18328,    449,    264,   5357,    389,  15374,\n",
       "             11,   3477,    279,  72758,   1495,    311,   8417,    422,    433,\n",
       "          15407,    279,  46092,  84614,   6926,    369,    264,   3230,   3769,\n",
       "             13,  12540,   1778,   2038,    387,   3118,     11,   1160,   1193,\n",
       "            279,   3769,   6532,  16662,   1202,  20081,  46092,  84614,   6926,\n",
       "          16134,    524,  39031,  40171,   5783,  22974,    532,    423,   4341,\n",
       "            278,   9451,  89492,   3893,   5364,  13915,   7911,  16628,    320,\n",
       "             72,  32989,      8,    304,    958,  10546,   9473,    198,  23045,\n",
       "          50185,    320,   5484,     34,      8,  52170,     14,   7489,   6535,\n",
       "             90,    508,  32816,   6251,   6535,     90,   1399,  32816,     33,\n",
       "           6535,     90,    508,     92,   8693,     16,     13,    717,    198,\n",
       "          20211,   5738,  99725,     14,   7489,   6535,     90,    508,  32816,\n",
       "           6251,   6535,     90,   1399,  32816,     33,   6535,     90,    508,\n",
       "             92,   8693,     16,     13,    717,  26807,      8,   6067,    617,\n",
       "           1027,  20041,    198,   1820,  18673,   2740,    323,   9526,    750,\n",
       "             13,  60288,   1113,   6205,  33297,  21037,    706,   1027,   1511,\n",
       "            198,    998,   6767,    872,  33297,   2065,    520,  50843,    323,\n",
       "            872,    958,  10546,   9473,    198,  23045,  12127,  18508,     13,\n",
       "           4314,  15629,    527,   1766,    311,    387,    315,    459,   3276,\n",
       "          11691,    442,  39100,   7138,    198,   2000,    279,  27313,  29014,\n",
       "           2134,  26839,    320,     15,     13,     20,     12,     16,  26807,\n",
       "            570,  11205,   8915,  24924,    198,  13495,   1051,  20041,   1701,\n",
       "          67744,    283,    258,   3177,  72916,    320,     33,   7416,      8,\n",
       "          15105,     13,    578,    198,     33,   7416,  22323,  16805,  38617,\n",
       "           2536,  60272,  49889,   5531,  12903,  17301,  54743,     13,    763,\n",
       "            198,  85324,    311,    279,  29217,    369,   8045,  59402,    358,\n",
       "           7650,   3623,   6251,     33,  13931,     11,    420,    198,  87803,\n",
       "           2536,   2827,  49889,   9103,    374,  29014,  26839,    323,   8617,\n",
       "          59086,   8333,    198,  38655,     13,  15636,     11,    311,  10552,\n",
       "            279,  22772,  17432,     11,    264,  32887,    198,   2590,   3196,\n",
       "            389,    279,  77933,   3834,    459,    285,    354,  18237,   6811,\n",
       "           1990,    279,    198,  15205,    323,   1948,   3623,   6251,     33,\n",
       "          13931,    706,   1027,   8040,     13,   1226,   1501,    430,    279,\n",
       "          29014,  26839,    198,  91746,    315,    279,  12903,  12330,   2536,\n",
       "          60272,  49889,   9103,    374,   1664,  55717,    555,  13126,    264,\n",
       "            198,  18568,    602,  32989,    323,   2204,  77933,    958,  22974,\n",
       "            532,    459,    285,    354,  18237,   5151,   1990,    198,   1820,\n",
       "           1948,    323,   5740,   3623,   6251,     33,  13931,     13,   1115,\n",
       "            459,    285,    354,  18237,   6811,    706,   1027,  11007,    198,\n",
       "           1729,    279,   8990,    315,    279,   3623,   6251,     33,  26839,\n",
       "          44393,    315,   7524,    198,     76,  64333,   2065,    315,  52170,\n",
       "             14,   7489,   6251,     33,  19945,     84,    323,  29014,     14,\n",
       "           7489,   6251,     33,  10482,     70,     46,   3927,  13931,     11,\n",
       "           1405,    264,  13790,    198,  30998,   9073,    706,   1027,  13468,\n",
       "             13,  66028,  65562,    933]], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('result', save_embedding_layers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "# for param in model.parameters():\n",
    "#     print(param.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(134933, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=128, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=128, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=128, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=128, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=134933, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     print(param.requires_grad, name, param.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify which parameters are trainable\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(f\"Trainable: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"Trainable: {name}\", param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.add_special_tokens({\"pad_token\": \"<PAD>\"})\n",
    "# model.resize_token_embeddings(len(tokenizer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
